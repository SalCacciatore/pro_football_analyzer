{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f2bdc669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score, brier_score_loss, mean_squared_error\n",
    "from sklearn.calibration import CalibrationDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3755c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\2001587006.py:33: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['turnover'] = data['interception'] + data['fumble_lost']\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\2001587006.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data['turnover'] = data['interception'] + data['fumble_lost']\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\2001587006.py:35: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['inside_10'] = (data['yardline_100'] < 10).astype(int)\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\2001587006.py:36: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['20+_play'] = (data['yards_gained'] > 19).astype(int)\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\2001587006.py:37: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['short_pass'] = (data['air_yards'] < 10).astype(int)\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\2001587006.py:38: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['medium_pass'] = ((data['air_yards'] > 9)&(data['air_yards']<20)).astype(int)\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\2001587006.py:39: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['deep_pass'] = (data['air_yards'] > 19).astype(int)\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\2001587006.py:40: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['end_zone_target'] = (data['yardline_100'] - data['air_yards']) <= 0\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\2001587006.py:41: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['fantasy_points'] = (\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\2001587006.py:46: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['distance_to_EZ_after_target'] = data['yardline_100'] - data['air_yards']\n"
     ]
    }
   ],
   "source": [
    "YEARS = [2018, 2019, 2020, 2021, 2022, 2023,2024]\n",
    "\n",
    "# %%\n",
    "data_all = pd.DataFrame()\n",
    "\n",
    "def calculate_seconds(row):\n",
    "    if row['qtr'] != 5:\n",
    "        return 3600 - row['game_seconds_remaining']\n",
    "    else:\n",
    "        return 600 - row['game_seconds_remaining'] + 3600\n",
    "\n",
    "\n",
    "def get_quarter_value(dataf):\n",
    "    if 'END QUARTER' in dataf['desc']:\n",
    "        return dataf['level_0']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "for i in YEARS:  \n",
    "    i_data = pd.read_csv('https://github.com/nflverse/nflverse-data/releases/download/pbp/' \\\n",
    "                   'play_by_play_' + str(i) + '.csv.gz',\n",
    "                   compression= 'gzip', low_memory= False)\n",
    "\n",
    "    data_all = pd.concat([data_all,i_data])\n",
    "\n",
    "ppr = 1\n",
    "\n",
    "data = data_all.loc[data_all.season_type=='REG']\n",
    "#data = data_all.loc[(data_all.play_type.isin(['no_play','pass','run'])) & (data_all.epa.isna()==False)]\n",
    "#data.loc[data['pass']==1, 'play_type'] = 'pass'\n",
    "#data.loc[data.rush==1, 'play_type'] = 'run'\n",
    "data.reset_index(drop=True, inplace=True)\n",
    "data['turnover'] = data['interception'] + data['fumble_lost']\n",
    "data = data.dropna(subset=['posteam'])\n",
    "data['inside_10'] = (data['yardline_100'] < 10).astype(int)\n",
    "data['20+_play'] = (data['yards_gained'] > 19).astype(int)\n",
    "data['short_pass'] = (data['air_yards'] < 10).astype(int)\n",
    "data['medium_pass'] = ((data['air_yards'] > 9)&(data['air_yards']<20)).astype(int)\n",
    "data['deep_pass'] = (data['air_yards'] > 19).astype(int)\n",
    "data['end_zone_target'] = (data['yardline_100'] - data['air_yards']) <= 0\n",
    "data['fantasy_points'] = (\n",
    "    data['complete_pass'] * ppr +          # 1 point per completion\n",
    "    data['touchdown'] * 6 +           # 6 points per touchdown\n",
    "    data['yards_gained'] * 0.1        # 0.1 points per yard gained\n",
    ")\n",
    "data['distance_to_EZ_after_target'] = data['yardline_100'] - data['air_yards']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "eb50b0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_finder(home_or_away,home_total,away_total):\n",
    "    if home_or_away == 'home':\n",
    "        total = home_total\n",
    "    else:\n",
    "        total = away_total \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2bce6116",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\256212464.py:7: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['home_implied_total'] = abs(data['total_line'] / 2 + data['spread_line'] / 2)\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\256212464.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['away_implied_total'] = abs(data['total_line'] / 2 - data['spread_line'] / 2)\n",
      "C:\\Users\\src15\\AppData\\Local\\Temp\\ipykernel_22924\\256212464.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  data['implied_posteam_total'] = [\n"
     ]
    }
   ],
   "source": [
    "data.reset_index(drop=True, inplace=True)\n",
    "\n",
    "data = data[data['two_point_attempt']==0]\n",
    "\n",
    "\n",
    " # derive implied team total from betting market data\n",
    "data['home_implied_total'] = abs(data['total_line'] / 2 + data['spread_line'] / 2)\n",
    "data['away_implied_total'] = abs(data['total_line'] / 2 - data['spread_line'] / 2)\n",
    "\n",
    "    # Use list comprehension with zip for more efficient row-wise operations\n",
    "data['implied_posteam_total'] = [\n",
    "    total_finder(has_ball, home_number, away_number)\n",
    "        for has_ball, home_number, away_number in zip(data['posteam_type'], data['home_implied_total'], data['away_implied_total'])\n",
    "]\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "    # we only want throws to a receiver, aka plays with air yardage (no running plays, sacks, throwaways etc.)\n",
    "throws = data[data['air_yards'].notna()]\n",
    "\n",
    "# only data before the current szn\n",
    "throws = throws[throws['season']!=2024]\n",
    "throws = throws[throws['receiver_player_name'].notna()]\n",
    "throws = throws[throws['pass_location'].notna()]\n",
    "\n",
    "    \n",
    "df = throws[['receiver_player_name','receiver_player_id','posteam','pass','cp','game_id','complete_pass','inside_10','air_yards','yardline_100','ydstogo','implied_posteam_total','yards_gained','fantasy_points','pass_touchdown','down','pass_location','week','season','home_implied_total','away_implied_total','posteam_type','qb_hit','end_zone_target', 'distance_to_EZ_after_target']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "06badee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Evaluation:\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 3, 'min_child_weight': 3, 'n_estimators': 100, 'subsample': 1.0}\n",
      "RMSE: 0.2133\n",
      "ROC-AUC Score: 0.5633\n",
      "\n",
      "Probability Calibration Check:\n",
      "\n",
      "Probability range 0.0-0.1 (n=20655):\n",
      "Average predicted: 0.000\n",
      "Actual observed: 0.041\n",
      "\n",
      "Feature Importance:\n",
      "                       Feature  Importance\n",
      "7  distance_to_EZ_after_target    0.733461\n",
      "1                 yardline_100    0.147146\n",
      "0                    air_yards    0.044500\n",
      "5                       qb_hit    0.018050\n",
      "2                      ydstogo    0.015871\n",
      "6              end_zone_target    0.010980\n",
      "3                         down    0.009846\n",
      "9          pass_location_right    0.007830\n",
      "8         pass_location_middle    0.007139\n",
      "4                       season    0.005176\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load and prepare data (same as before)\n",
    "predictors = [\n",
    "    'air_yards', 'yardline_100', 'ydstogo',\n",
    "    'down', 'pass_location', 'season', 'qb_hit', 'end_zone_target', 'distance_to_EZ_after_target'\n",
    "]\n",
    "X = df[predictors]\n",
    "y = df['pass_touchdown']\n",
    "\n",
    "# Convert categorical variables\n",
    "X = pd.get_dummies(X, columns=['pass_location'], drop_first=True)\n",
    "X.fillna(X.mean(), inplace=True)\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# classififer\n",
    "model = xgb.XGBClassifier(\n",
    "    objective='binary:logistic',\n",
    "    eval_metric=['logloss', 'auc']\n",
    ")\n",
    "\n",
    "# Hyperparameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'subsample': [0.8, 1.0],\n",
    "    'colsample_bytree': [0.8, 1.0],\n",
    "    'min_child_weight': [1, 3]\n",
    "}\n",
    "\n",
    "# Grid search with both MSE and ROC-AUC scoring\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=model,\n",
    "    param_grid=param_grid,\n",
    "    scoring=['neg_mean_squared_error', 'roc_auc'],\n",
    "    refit='neg_mean_squared_error',  # Choose MSE as primary metric\n",
    "    cv=5\n",
    ")\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best model\n",
    "touchdown_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions\n",
    "y_pred = touchdown_model.predict(X_test)\n",
    "\n",
    "# Evaluate both regression and classification metrics\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "print(\"Model Evaluation:\")\n",
    "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "print(f\"RMSE: {rmse:.4f}\")\n",
    "print(f\"ROC-AUC Score: {roc_auc:.4f}\")\n",
    "\n",
    "# Check calibration across probability ranges\n",
    "prob_ranges = np.linspace(0, 1, 11)\n",
    "print(\"\\nProbability Calibration Check:\")\n",
    "for i in range(len(prob_ranges)-1):\n",
    "    mask = (y_pred >= prob_ranges[i]) & (y_pred < prob_ranges[i+1])\n",
    "    if mask.any():\n",
    "        avg_pred_prob = y_pred[mask].mean()\n",
    "        actual_prob = y_test[mask].mean()\n",
    "        n_samples = mask.sum()\n",
    "        print(f\"\\nProbability range {prob_ranges[i]:.1f}-{prob_ranges[i+1]:.1f} (n={n_samples}):\")\n",
    "        print(f\"Average predicted: {avg_pred_prob:.3f}\")\n",
    "        print(f\"Actual observed: {actual_prob:.3f}\")\n",
    "\n",
    "# Feature importance\n",
    "importance = touchdown_model.feature_importances_\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Importance': importance\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "print(\"\\nFeature Importance:\")\n",
    "print(feature_importance_df)\n",
    "\n",
    "# Example prediction function\n",
    "def predict_touchdown_probability(model, play_data):\n",
    "    \"\"\"\n",
    "    Make touchdown probability predictions for new plays.\n",
    "    \"\"\"\n",
    "    # Ensure play_data has same preprocessing as training data\n",
    "    play_data = pd.get_dummies(play_data, columns=['pass_location'], drop_first=True)\n",
    "    \n",
    "    # Add any missing columns from training data\n",
    "    for col in model.feature_names_in_:\n",
    "        if col not in play_data.columns:\n",
    "            play_data[col] = 0\n",
    "            \n",
    "    # Reorder columns to match training data\n",
    "    play_data = play_data[model.feature_names_in_]\n",
    "    \n",
    "    # Get predictions and ensure they're in [0,1]\n",
    "    probabilities = model.predict(play_data)\n",
    "    probabilities = np.clip(probabilities, 0, 1)\n",
    "    \n",
    "    return probabilities\n",
    "\n",
    "\n",
    "# Print top factors influencing this prediction\n",
    "def explain_prediction(model, play_data, feature_importance_df):\n",
    "    \"\"\"Explain what factors most influenced a specific prediction\"\"\"\n",
    "    # Get the top 5 most important features\n",
    "    top_features = feature_importance_df.head()\n",
    "    \n",
    "    print(\"\\nTop factors influencing this prediction:\")\n",
    "    for _, row in top_features.iterrows():\n",
    "        feature = row['Feature']\n",
    "        if feature in play_data.columns:\n",
    "            value = play_data[feature].iloc[0]\n",
    "            print(f\"{feature}: {value} (importance: {row['Importance']:.3f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "59469a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: .2133\n"
     ]
    }
   ],
   "source": [
    "print('RMSE: .2133')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "420dfa1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "\n",
    "with open('pass_touchdown_model.pkl', 'wb') as file:\n",
    "    pickle.dump(touchdown_model, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "57dc2986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
